History of women in the United States
This is a piece on history of women in the United States since 1776, and of the Thirteen Colonies before that. The study of women's history has been a major scholarly and popular field, with many scholarly books and articles, museum exhibits, and courses in schools and universities. The roles of women were long ignored in textbooks and popular histories. By the 1960s, women were being presented as successful as male roles. An early feminist approach underscored their victimization and inferior status at the hands of men. In the 21st century writers have emphasized the distinctive strengths displayed inside the community of women, with special concern for minorities among women. Colonial era The experiences of women during the colonial era varied from colony to colony, but there were some overall patterns. Most of the British settlers were from England and Wales, with smaller numbers from Scotland and Ireland. Groups of families settled together in New England, while families tended to settle independently in the Southern colonies. The American colonies absorbed several thousands of Dutch and Swedish settlers. After 1700, most immigrants to Colonial America arrived as indentured servantsâ€”young unmarried men and women seeking a new life in a much richer 
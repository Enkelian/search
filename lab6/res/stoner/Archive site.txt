Archive site
In web archiving, an archive site is a website that stores information on webpages from the past for anyone to view. Common techniques Two common techniques for archiving websites are using a web crawler or soliciting user submissions: Using a web crawler: By using a web crawler (e.g., the Internet Archive) the service will not depend on an active community for its content, and thereby can build a larger database faster. However, web crawlers are only able to index and archive information the public has chosen to post to the Internet, or that is available to be crawled, as website developers and system administrators have the ability to block web crawlers from accessing [certain] web pages (using a robots.txt). User submissions: While it can be difficult to start user submission services due to potentially low rates of user submissions, this system can yield some of the best results. By crawling web pages one is only able to obtain the information the public has chosen to post online; however, potential content providers may not bother to post certain information, assuming no one would be interested in it, because they lack a proper venue in which to post it, or because of 
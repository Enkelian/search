Evangelicalism in the United States
In the United States, evangelicalism is an umbrella group of Protestant Christians who believe in the necessity of being born again, emphasize the importance of evangelism, and affirm traditional Protestant teachings on the authority and the historicity of the Bible. Nearly a quarter of the US population, evangelicals are diverse and drawn from a variety of denominational backgrounds, including Baptist, Mennonite, Methodist, Holiness, Pentecostal, Reformed and nondenominational churches.Evangelicalism has played an important role in shaping American religion and culture. The First Great Awakening of the 18th century marked the rise of evangelical religion in colonial America. As the revival spread throughout the Thirteen Colonies, evangelicalism united Americans around a common faith. The Second Great Awakening of the 19th century led to what historian Martin Marty called the "Evangelical Empire", a period in which evangelicals dominated US cultural institutions, including schools and universities. Evangelicals in the northern United States were strong advocates of reform. They were involved in the temperance movement and supported the abolition of slavery in addition to working towards education and criminal justice reform. In the southern United States, evangelicals split from their northern counterparts on the issue of slavery, establishing new denominations that did not call for 